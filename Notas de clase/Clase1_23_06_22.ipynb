{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clase1_23-06-22.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cleyf-CHzqh1"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 4 clases Eze (8 a 11)\n",
        "- 4 clases Marcos (horario?)\n",
        "\n",
        "Libro: Deep Learning - Ian Goodfellow (https://deeplearningbook.org)\n",
        "\n",
        "Para definir un modelo ML:\n",
        "- Loss function (funcion de costo). No se elige aleatoreamente\n",
        "- Modelo. Como se relaciona la entrada con la salida\n",
        "- Opmizador. Como aprende el modelo a partir de los datos que observa\n",
        "\n",
        "AI -> Machine Learning -> Representation learning -> Deep Learning\n",
        "\n",
        "Diferencias en como funciona el software clasico (entre la entrada y la salida hay codigo hecho a mano), machine-learning clasico (hay un modelo entre entrada y salida pero los features son clasificados a mano) y deep learning (aprende end-to-end y de forma composicional)\n",
        "\n",
        "Ejemplo de deep learning: Problema de recomendacion de peliculas de Netflix.\n",
        "\n",
        "- **Por que necesito modelos no lineales?** Ejemplo simple hecho a mano (XOR) tratado como un problema de regresion con entradas x1, x2 y salida y. Resolvemos como (1)modelo lineal y (2)modelo no lineal\n",
        "\n",
        "(1) Modelo lineal: regresion lineal\n",
        "Loss function: MSE\n",
        "Optimizador: gradiente"
      ],
      "metadata": {
        "id": "2GEnAERTzxBb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0, 0, 1], [0, 1, 1], [1, 0, 1], [1, 1, 1]])\n",
        "Y = np.array([0, 1, 1, 0]).reshape(-1, 1)\n",
        "\n",
        "W = np.linalg.inv(X.T @ X) @ X.T @ Y\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtiRUmWhD9B7",
        "outputId": "1da22ac5-009f-4024-ec0a-bed79c172d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00],\n",
              "       [2.22044605e-16],\n",
              "       [5.00000000e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "El resultado es una constante (w1=w2=0 y b=0.5) de manera que es una solucion bastante mala la que da la regresion lineal.\n",
        "\n",
        "* **Introduciendo la neurona**. Tiene una componente lineal Z_i,1 y una componente alineal a_i,1 (funcion de activacion). En general el layer lineal esta compuesto por varias neuronas. Puede haber redes con solo un layer lineal y uno alineal o redes con multiples layers intercalados, usando distintos tipos de layers (en lugar de lineal, puede ser convolutional, recurrent o sigmoid, softmax, ReLu para activacion)\n",
        "\n",
        "O = cantidad de neuronas\n",
        "M = cantidad de inputs\n",
        "\n",
        "Dim(A) = Ox1\n",
        "Dim(W) = Oxm\n",
        "Dim(x_i) = mx1\n",
        "Dim(b) = Ox1\n",
        "\n",
        "* Bloques de red con p layers:\n",
        "1) Capa 1 lineal, convolutional, recurrent, etc\n",
        "2) Capa de activacion alineal sigmoide, ReLu, tanh, etc\n",
        "3) Capa 2 lineal, conv...\n",
        "4) Capa 2 de activacion...\n",
        ".\n",
        ".\n",
        ".\n",
        "P) Capa output\n",
        "P+1) Capa de Loss (en conjunto con la capa de output se eligen de manera de que resuelvan optimamente el problema, por ejemplo, Sigmoid+BCE se utiliza en clasificacion binaria, Softmax+CE en clasificacion multiclase, MSE para regresion, etc)\n",
        "Backpropagation) Se inyecta el resultado de la red a la entrada\n",
        "\n",
        "Forward: Resolucion de las capas lineales y de activacion hasta el output.\n",
        "Backpropagation: Reutilizar la salida de la red despues de la funcion de perdida y reinyectarla a la entrada\n",
        "\n",
        "* **Resolviendo el ejemplo de la XOR con una red neuronal** Usamos una capa lineal1+activacion1 con dimensiones in=2, out=2 y parametros=6 (4 conexiones y 1 bias por cada neurona) y una segunda capa lineal2+activacion2 con dimensiones in=2, out=1, parametros=3 (en total tenemos 9 parametros)\n",
        "\n",
        "Forward: \n",
        "* z_i,1(1) = w_1,1(1) + w_1,2(1) + b_1(1)\n",
        "* z_i,1(1) = w_2,1(1) + w_2,2(1) + b_2(1) \n",
        "* a_i,1(1) = sig(Z_i,1(1))\n",
        "* a_i,2(1) = sig(Z_i,2(1))\n",
        "* Z_i,1(2) = w_i,1(2) . a_i,1(1) + w_i,2(2) . a_i,2(1) + b_1(2)\n",
        "* yhat_i = z_i,1(2)\n",
        "* L_w = (y_i-yhat_i)^2\n",
        "\n",
        "**SGD (stochastic gradient descent)** Se calcula el gradiente y se avanza en sentido inverso al gradiente en pasos de learning rate en cada epoch buscando el valor que minimiza la funcion de perdida:\n",
        "* El algoritmo de SGD inicializa los w y b de alguna manera, por ejemplo con una uniforme U(0, 1)\n",
        "* for epoch in range(n_epochs, siendo n_epochs un hiperparametro, por ejemplo 100)\n",
        "* for x_i in input\n",
        "* Forward -> y_i = fhat(x_i), donde fhay=NNET\n",
        "* Error -> err = (y_i - yhat_i)\n",
        "* Backpropagation\n",
        "* Se actualizan los parametros -> w_1,1(1) = w_1,1(1) - alpha.dL/d((w_1,1)(1) ...., siendo alpha (learning rate) otro hiperparametro (por ejemplo 0.01) \n",
        "* Se calcula MSE\n",
        "* Se guardan valores para luego graficar"
      ],
      "metadata": {
        "id": "WU5SBEEXGuEA"
      }
    }
  ]
}